# Pipeline de Dados - Arquitetura Medallion

## Vis√£o Geral

O pipeline de dados implementa a **Arquitetura Medallion** (Bronze ‚Üí Silver ‚Üí Gold) no Databricks, transformando dados brutos da CVM em informa√ß√µes anal√≠ticas prontas para consumo.

## Camadas de Dados

### üü§ Bronze Layer - Dados Brutos

#### Prop√≥sito
- Ingest√£o de dados brutos diretamente do S3
- Preserva√ß√£o hist√≥rica completa
- Schema on read (sem valida√ß√µes)

#### Implementa√ß√£o

```python
df_bronze = (spark.read.format("csv")
    .option("header", "true")
    .option("inferSchema", "true")
    .option("delimiter", ";")
    .option("encoding", "ISO-8859-1")
    .option("fs.s3a.access.key", access_key)
    .option("fs.s3a.secret.key", secret_key)
    .option("fs.s3a.session.token", session_token)
    .option("fs.s3a.aws.credentials.provider", 
            "org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider")
    .load(path_bronze_csv)
)

# Salva como tabela Delta
df_bronze.write.format("delta") \
    .mode("overwrite") \
    .option("overwriteSchema", "true") \
    .saveAsTable("cvm_p210.bronze_inf_diario")
```

#### Caracter√≠sticas
- ‚úÖ **Formato**: Delta Lake
- ‚úÖ **Schema**: Inferido automaticamente
- ‚úÖ **Encoding**: ISO-8859-1 (padr√£o CVM)
- ‚úÖ **Separador**: `;` (ponto e v√≠rgula)

---

### ‚ö™ Silver Layer - Dados Limpos e Padronizados

#### Prop√≥sito
- Limpeza e padroniza√ß√£o de dados
- Aplica√ß√£o de regras de qualidade
- Tratamento de evolu√ß√£o de schema
- Deduplica√ß√£o

#### Transforma√ß√µes Aplicadas

##### 1. Tratamento de Colunas (Compatibilidade CVM 175)

```python
# Compatibilidade entre formatos antigo e novo da CVM
df_silver = df_bronze.withColumn(
    "CNPJ_FUNDO",
    coalesce(col("CNPJ_FUNDO"), col("cnpj_fundo"))
)
```

**Motivo:** A CVM alterou padr√£o de nomenclatura em algumas publica√ß√µes.

##### 2. Convers√£o de Tipos de Dados

```python
df_silver = df_silver \
    .withColumn("DT_COMPTC", to_date(col("DT_COMPTC"), "yyyy-MM-dd")) \
    .withColumn("VL_TOTAL", col("VL_TOTAL").cast("double")) \
    .withColumn("VL_QUOTA", col("VL_QUOTA").cast("double")) \
    .withColumn("VL_PATRIM_LIQ", col("VL_PATRIM_LIQ").cast("double")) \
    .withColumn("CAPTC_DIA", col("CAPTC_DIA").cast("double")) \
    .withColumn("RESG_DIA", col("RESG_DIA").cast("double"))
```

##### 3. Enriquecimento de Dados

```python
df_silver = df_silver \
    .withColumn("ano", year(col("DT_COMPTC"))) \
    .withColumn("mes", month(col("DT_COMPTC"))) \
    .withColumn("dh_processamento_silver", current_timestamp())
```

**Metadados adicionados:**
- `ano`: Particionamento
- `mes`: Particionamento
- `dh_processamento_silver`: Rastreabilidade

##### 4. Data Quality - Filtros

```python
df_silver = df_silver.filter("VL_PATRIM_LIQ > 0")
```

**Regra:** Remove registros com patrim√¥nio l√≠quido inv√°lido.

#### Estrat√©gia de Merge (Deduplica√ß√£o)

```python
if DeltaTable.isDeltaTable(spark, "cvm_p210.silver_inf_diario"):
    delta_table = DeltaTable.forName(spark, "cvm_p210.silver_inf_diario")
    
    delta_table.alias("target").merge(
        df_silver.alias("source"),
        "target.CNPJ_FUNDO = source.CNPJ_FUNDO AND target.DT_COMPTC = source.DT_COMPTC"
    ).whenMatchedUpdateAll() \
     .whenNotMatchedInsertAll() \
     .execute()
else:
    df_silver.write.format("delta") \
        .partitionBy("ano", "mes") \
        .saveAsTable("cvm_p210.silver_inf_diario")
```

**Garantia:** N√£o h√° duplicatas para o mesmo `CNPJ_FUNDO + DT_COMPTC`.

#### Otimiza√ß√£o

```sql
OPTIMIZE cvm_p210.silver_inf_diario ZORDER BY (CNPJ_FUNDO)
```

**Benef√≠cio:** Consultas filtradas por CNPJ_FUNDO s√£o at√© **10x mais r√°pidas**.

---

### üü° Gold Layer - Dados Anal√≠ticos

#### Prop√≥sito
- Agrega√ß√µes por fundo e per√≠odo
- C√°lculo de KPIs de neg√≥cio
- Dados prontos para BI e an√°lises

#### Regras de Neg√≥cio Implementadas

##### 1. Agrega√ß√µes Base

```python
df_gold_base = df_silver.groupBy("CNPJ_FUNDO", "ano", "mes").agg(
    count("*").alias("dias_negociacao"),
    sum("CAPTC_DIA").alias("total_captacao"),
    sum("RESG_DIA").alias("total_resgate"),
    avg("VL_PATRIM_LIQ").alias("patrimonio_medio"),
    max("VL_QUOTA").alias("cota_maxima"),
    min("VL_QUOTA").alias("cota_minima")
)
```

##### 2. KPIs Calculados

```python
df_gold_insights = df_gold_base \
    .withColumn("fluxo_liquido", 
                round(col("total_captacao") - col("total_resgate"), 2)) \
    .withColumn("variacao_cota_mes", 
                round(((col("cota_maxima") - col("cota_minima")) / col("cota_minima")) * 100, 2))
```

**KPIs Criados:**
- **Fluxo L√≠quido**: Indica se houve entrada ou sa√≠da de capital
  - `> 0`: Capta√ß√£o l√≠quida (positivo para o fundo)
  - `< 0`: Resgate l√≠quido (portabilidade de sa√≠da)
- **Varia√ß√£o de Cota**: Performance do fundo no per√≠odo

##### 3. Metadados Anal√≠ticos

```python
df_gold_insights = df_gold_insights \
    .withColumn("dh_geracao_analytics", current_timestamp()) \
    .withColumn("versao_pipeline", lit("1.0"))
```

#### Escrita na Camada Gold

```python
df_gold_insights.write.format("delta") \
    .mode("overwrite") \
    .saveAsTable("cvm_p210.gold_cvm210_analytics")
```

---

## Governan√ßa e Metadados

### Unity Catalog

Todas as tabelas s√£o criadas no **Unity Catalog** do Databricks:

```
Catalog: cvm_p210
‚îú‚îÄ‚îÄ bronze_inf_diario
‚îú‚îÄ‚îÄ silver_inf_diario
‚îî‚îÄ‚îÄ gold_cvm210_analytics
```

**Benef√≠cios:**
- üìö **Cat√°logo centralizado** de metadados
- üîí **Controle de acesso** granular
- üìä **Data lineage** autom√°tico

### Versionamento (Delta Lake)

#### Time Travel

```sql
-- Ver vers√£o anterior da tabela
SELECT * FROM cvm_p210.silver_inf_diario VERSION AS OF 5

-- Ver tabela em data espec√≠fica
SELECT * FROM cvm_p210.silver_inf_diario TIMESTAMP AS OF '2026-01-10'
```

#### Hist√≥rico de Vers√µes

```sql
DESCRIBE HISTORY cvm_p210.silver_inf_diario
```

---

## Execu√ß√£o do Pipeline

### Ordem de Execu√ß√£o

1. **Bronze**: Ingest√£o de dados brutos do S3
2. **Silver**: Limpeza, padroniza√ß√£o e merge
3. **Gold**: Agrega√ß√µes e c√°lculo de KPIs

### Idempot√™ncia

O pipeline √© **idempotente**:
- M√∫ltiplas execu√ß√µes do mesmo per√≠odo **n√£o criam duplicatas**
- Merge garante `UPSERT` (atualiza se existe, insere se n√£o existe)

---

## Monitoramento e Qualidade

### Data Quality Checks

| Check | Descri√ß√£o | A√ß√£o |
|-------|-----------|------|
| Patrim√¥nio > 0 | Valida patrim√¥nio l√≠quido positivo | Remove registros inv√°lidos |
| Campos obrigat√≥rios | CNPJ_FUNDO, DT_COMPTC n√£o nulos | Garantido pela merge key |
| Duplicatas | Chave (CNPJ + Data) √∫nica | Merge evita duplica√ß√£o |

### Logs de Processamento

Cada camada registra timestamp de processamento:
- `dh_processamento_silver`: Quando foi processado na Silver
- `dh_geracao_analytics`: Quando foi gerado na Gold

---

## Pr√≥ximas Melhorias

> [!NOTE]
> **Evolu√ß√£o do Pipeline**

- [ ] **Valida√ß√£o de schema** antes da ingest√£o
- [ ] **Alertas de Data Quality** (ex: SNS quando patrim√¥nio m√©dio cai muito)
- [ ] **M√©tricas de pipeline** (tempo de execu√ß√£o, volume de dados)
- [ ] **Testes automatizados** (Great Expectations)
- [ ] **Orquestra√ß√£o completa** (Databricks Workflows ou Airflow)

---

## C√≥digo Completo

[Ver notebook_principal.ipynb](file:///c:/Users/Usuario/.gemini/antigravity/scratch/eng-dados-project/notebook_principal.ipynb)
