# CVM 210 Data Pipeline

![Project Banner](assets/images/project_banner.png)

[![AWS](https://img.shields.io/badge/AWS-Lambda%20%7C%20S3-FF9900?style=for-the-badge&logo=amazon-aws&logoColor=white)](https://aws.amazon.com/)
[![Databricks](https://img.shields.io/badge/Databricks-Delta%20Lake-FF3621?style=for-the-badge&logo=databricks&logoColor=white)](https://databricks.com/)
[![Python](https://img.shields.io/badge/Python-3.9%2B-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://www.python.org/)
[![PySpark](https://img.shields.io/badge/PySpark-E25A1C?style=for-the-badge&logo=apache-spark&logoColor=white)](https://spark.apache.org/)
[![License](https://img.shields.io/badge/License-MIT-green?style=for-the-badge)](LICENSE)
[![Code Style](https://img.shields.io/badge/Code%20Style-Black-black?style=for-the-badge)](https://github.com/psf/black)

## ğŸ“‘ Ãndice

- [VisÃ£o Geral](#-visÃ£o-geral)
- [Objetivos do Projeto](#-objetivos-do-projeto)
- [Arquitetura da SoluÃ§Ã£o](#%EF%B8%8F-arquitetura-da-soluÃ§Ã£o)
- [Tecnologias Utilizadas](#-tecnologias-utilizadas)
- [Estrutura do Projeto](#-estrutura-do-projeto)
- [Como Executar](#-como-executar)
- [Exemplos de AnÃ¡lises](#-exemplos-de-anÃ¡lises)
- [GovernanÃ§a e Qualidade](#-regras-de-negÃ³cio--governanÃ§a)
- [OtimizaÃ§Ã£o de Custos](#-otimizaÃ§Ã£o-de-custos)
- [MÃ©tricas de Performance](#-mÃ©tricas-de-performance)
- [Troubleshooting](#-troubleshooting)
- [PrÃ³ximos Passos](#-prÃ³ximos-passos)
- [Diferenciais do Projeto](#-diferenciais-do-projeto)
- [Sobre o Autor](#-sobre-o-autor)
- [LicenÃ§a](#-licenÃ§a)

---

## ğŸ“Œ VisÃ£o Geral

Este projeto implementa uma **soluÃ§Ã£o completa de engenharia de dados** para capturar, processar e analisar informaÃ§Ãµes pÃºblicas da **CVM (ResoluÃ§Ã£o CVM 210)**, simulando um cenÃ¡rio real de mercado onde nÃ£o hÃ¡ API oficial disponÃ­vel para consumo direto dos dados regulatÃ³rios.

> [!IMPORTANT]
> **Contexto RegulatÃ³rio**: Embora a CVM disponibilize dados pÃºblicos em seu portal, **as informaÃ§Ãµes da ResoluÃ§Ã£o CVM 210 (portabilidade de investimentos) sÃ£o de acesso restrito** e nÃ£o estÃ£o disponÃ­veis publicamente. Este projeto demonstra a capacidade de construir uma arquitetura robusta para captura e anÃ¡lise desses dados em um ambiente corporativo real.

A proposta foi resolver um problema real de negÃ³cio, desde a **ingestÃ£o automatizada** atÃ© a **anÃ¡lise estratÃ©gica**, utilizando uma arquitetura moderna em cloud (**AWS + Databricks**) e boas prÃ¡ticas de Data Engineering em produÃ§Ã£o.

---

## ğŸ¯ Objetivos do Projeto

âœ… **Automatizar a captura diÃ¡ria** dos dados publicados pela CVM  
âœ… **Garantir persistÃªncia, histÃ³rico e rastreabilidade** das informaÃ§Ãµes  
âœ… **Transformar dados brutos em informaÃ§Ã£o analÃ­tica confiÃ¡vel**  
âœ… **Viabilizar anÃ¡lises de portabilidade e movimentaÃ§Ã£o de fundos**  
âœ… **Criar uma base escalÃ¡vel** para estratÃ©gias financeiras futuras

---

## ğŸ—ï¸ Arquitetura da SoluÃ§Ã£o

![Architecture Diagram](assets/images/architecture_diagram.png)

### Fluxo Completo de Dados

```mermaid
graph TB
    A[CVM Website<br/>inf_diario_fi_YYYYMM.zip] -->|HTTP Download| B[AWS Lambda<br/>Daily Trigger]
    B -->|Unzip + Upload| C[S3 Data Lake<br/>ano=YYYY/mes=MM/]
    C -->|PySpark Read| D{Databricks<br/>ELT Processing}
    D -->|Ingestion<br/>Schema on Read| E[(Bronze Layer<br/>Raw CSV Data)]
    E -->|Transformations<br/>Quality Checks| F[(Silver Layer<br/>Cleaned + Standardized)]
    F -->|Aggregations<br/>Business KPIs| G[(Gold Layer<br/>Analytics Ready)]
    G -->|SQL Queries| H[Portability Analysis<br/>Risk Detection]
    H -.->|Future: Alerts| I[CRM Integration<br/>Planned]
    I -.->|Future: Email| J[Investment Advisors<br/>Planned]
    
    style A fill:#e1f5ff,stroke:#0066cc,stroke-width:2px
    style B fill:#fff4e1,stroke:#ff9900,stroke-width:2px
    style C fill:#ffe1f5,stroke:#cc00cc,stroke-width:2px
    style D fill:#ffe1f5,stroke:#cc00cc,stroke-width:2px
    style E fill:#d4a574,stroke:#8b5a2b,stroke-width:2px
    style F fill:#c0c0c0,stroke:#666666,stroke-width:2px
    style G fill:#ffd700,stroke:#cc9900,stroke-width:2px
    style H fill:#c1f0c1,stroke:#00aa00,stroke-width:2px
    style I fill:#d4d4ff,stroke:#6666ff,stroke-width:2px,stroke-dasharray: 5 5
    style J fill:#f0e1ff,stroke:#9966cc,stroke-width:2px,stroke-dasharray: 5 5
```

> **Legenda**: Componentes com borda tracejada e setas pontilhadas representam funcionalidades planejadas (Future State).

### ğŸ”¹ IngestÃ£o de Dados

- **ExecuÃ§Ã£o diÃ¡ria via AWS Lambda**, alinhada ao horÃ¡rio de publicaÃ§Ã£o da CVM
- **ExtraÃ§Ã£o direta do site da CVM** (simulando ingestÃ£o via API de instituiÃ§Ãµes autorizadas)
- **GeraÃ§Ã£o de arquivos diÃ¡rios no Amazon S3** (Data Lake â€“ Raw Zone)
- **DescompactaÃ§Ã£o automÃ¡tica** de arquivos ZIP para CSV

### ğŸ”¹ Armazenamento

- **OrganizaÃ§Ã£o dos dados no S3** com separaÃ§Ã£o lÃ³gica por data (`ano=YYYY/mes=MM/`)
- **PersistÃªncia histÃ³rica** para auditoria e reprocessamento
- **Base preparada para schema evolution**

### ğŸ”„ Processamento & TransformaÃ§Ã£o (ELT)

**Arquitetura Medallion implementada no Databricks:**

![Medallion Flow](assets/images/medallion_flow.png)

#### **Bronze Layer** ğŸŸ¤
- Dados brutos ingeridos diretamente do S3
- Schema on read
- Sem transformaÃ§Ãµes
- HistÃ³rico completo preservado

#### **Silver Layer** âšª
- **Limpeza e padronizaÃ§Ã£o** de dados
- **Tratamento de colunas** (compatibilidade CVM 175 vs formato antigo)
- **ValidaÃ§Ãµes de Data Quality** (valores vÃ¡lidos, campos obrigatÃ³rios)
- **DeduplicaÃ§Ã£o** via merge (garantia de nÃ£o duplicidade)
- **Schema evolution** automÃ¡tico
- **Z-Order optimization** por CNPJ_FUNDO

#### **Gold Layer** ğŸŸ¡
- **AgregaÃ§Ãµes por fundo e perÃ­odo**
- **KPIs de negÃ³cio calculados:**
  - Total de captaÃ§Ãµes
  - Total de resgates
  - **Fluxo lÃ­quido** (captaÃ§Ã£o - resgate)
  - PatrimÃ´nio mÃ©dio
  - VariaÃ§Ã£o de cota
  - **Indicadores de portabilidade**
- **Dados prontos para BI e anÃ¡lises**

### ğŸ”§ Tecnologias Utilizadas

| Camada | Tecnologia | PropÃ³sito |
|--------|-----------|-----------|
| **IngestÃ£o** | AWS Lambda + Python | AutomaÃ§Ã£o de captura diÃ¡ria |
| **Storage** | Amazon S3 | Data Lake (Raw Zone) |
| **Processing** | Databricks + PySpark | Processamento distribuÃ­do |
| **Data Format** | Delta Lake | Versionamento, ACID, Time Travel |
| **Orchestration** | AWS EventBridge (trigger diÃ¡rio) | Agendamento |
| **Governance** | Unity Catalog | CatÃ¡logo de metadados |

---

## ğŸ“‚ Estrutura do Projeto

```
eng-dados-project/
â”‚
â”œâ”€â”€ assets/
â”‚   â””â”€â”€ images/                   # Imagens do projeto (banner, diagramas)
â”‚       â”œâ”€â”€ project_banner.png
â”‚       â”œâ”€â”€ architecture_diagram.png
â”‚       â””â”€â”€ medallion_flow.png
â”‚
â”œâ”€â”€ notebooks/                     # Jupyter Notebooks
â”‚   â”œâ”€â”€ pipeline_principal.ipynb  # Pipeline completo (Bronze â†’ Silver â†’ Gold)
â”‚   â”œâ”€â”€ data_processing.ipynb     # Processamento e transformaÃ§Ãµes
â”‚   â””â”€â”€ analytics.ipynb           # AnÃ¡lises de portabilidade e insights
â”‚
â”œâ”€â”€ src/                          # CÃ³digo Python reutilizÃ¡vel
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ utils.py                  # FunÃ§Ãµes utilitÃ¡rias (formataÃ§Ã£o, validaÃ§Ã£o)
â”‚   â””â”€â”€ s3_helper.py              # Helper para operaÃ§Ãµes S3
â”‚
â”œâ”€â”€ docs/                         # DocumentaÃ§Ã£o tÃ©cnica
â”‚   â”œâ”€â”€ lambda_ingestion.md      # Detalhes da ingestÃ£o Lambda
â”‚   â”œâ”€â”€ data_pipeline.md         # Detalhes do pipeline ELT
â”‚   â””â”€â”€ analytics_guide.md       # Guia de anÃ¡lises disponÃ­veis
â”‚
â”œâ”€â”€ lambda_function.py            # FunÃ§Ã£o Lambda para ingestÃ£o diÃ¡ria
â”‚
â”œâ”€â”€ .gitignore                    # Arquivos ignorados pelo Git
â”œâ”€â”€ .env.example                  # Template de variÃ¡veis de ambiente
â”œâ”€â”€ requirements.txt              # DependÃªncias Python
â”œâ”€â”€ LICENSE                       # LicenÃ§a MIT
â””â”€â”€ README.md                     # Este arquivo
```

---

## ğŸš€ Como Executar

### 1ï¸âƒ£ ConfiguraÃ§Ã£o da IngestÃ£o (AWS Lambda)

```bash
# Deploy da funÃ§Ã£o Lambda
aws lambda create-function \
  --function-name cvm210-daily-ingestion \
  --runtime python3.9 \
  --role <IAM_ROLE_ARN> \
  --handler lambda_function.lambda_handler \
  --zip-file fileb://lambda_function.zip

# Configurar trigger diÃ¡rio (EventBridge)
aws events put-rule \
  --name cvm210-daily-trigger \
  --schedule-expression "cron(0 20 * * ? *)"
```

### 2ï¸âƒ£ Processamento no Databricks

1. **Configurar credenciais AWS** no Databricks (via Secrets ou IAM Role)
2. **Executar notebook `notebooks/pipeline_principal.ipynb`** para criar as camadas Bronze/Silver/Gold
3. **Executar notebook `notebooks/analytics.ipynb`** para anÃ¡lises de portabilidade

### 3ï¸âƒ£ Consultar Dados AnalÃ­ticos

```sql
-- Exemplo: Fundos com maior saÃ­da de capital (portabilidade)
SELECT 
  CNPJ_FUNDO,
  total_captacao,
  total_resgate,
  fluxo_liquido,
  patrimonio_medio
FROM cvm_p210.gold_cvm210_analytics
WHERE fluxo_liquido < 0
ORDER BY fluxo_liquido ASC
LIMIT 10;
```

---

## ğŸ“Š Exemplos de AnÃ¡lises

### AnÃ¡lise de Portabilidade

O projeto gera **insights acionÃ¡veis** para o time de negÃ³cios:

- **Fundos em risco** (com fluxo lÃ­quido negativo)
- **Volume total movimentado** por tipo de portabilidade
- **TendÃªncias de captaÃ§Ã£o vs resgate**

**Exemplo de output:**

| CNPJ_FUNDO | Total CaptaÃ§Ã£o | Total Resgate | Fluxo LÃ­quido | PatrimÃ´nio MÃ©dio |
|------------|----------------|---------------|---------------|------------------|
| 12.345.678 | R$ 1.2M | R$ 2.5M | **-R$ 1.3M** | R$ 450M |
| 23.456.789 | R$ 800K | R$ 1.1M | **-R$ 300K** | R$ 120M |

---

## ğŸ§  Regras de NegÃ³cio & GovernanÃ§a

### Data Quality
- âœ… ValidaÃ§Ã£o de **campos obrigatÃ³rios** (CNPJ_FUNDO, DT_COMPTC)
- âœ… Filtro de **patrimÃ´nio lÃ­quido > 0**
- âœ… **DeduplicaÃ§Ã£o automÃ¡tica** via merge

### Metadados e Rastreabilidade
- âœ… Timestamp de processamento em cada camada
- âœ… Particionamento por **ano/mÃªs**
- âœ… Versionamento via **Delta Lake**

### SeguranÃ§a & GovernanÃ§a
- ğŸ”’ **Credenciais AWS**: IAM Roles via Instance Profile (production-ready)
- ğŸ”’ **Armazenamento**: Unity Catalog para controle de acesso
- ğŸ”’ **Auditoria**: Delta Lake transaction log para rastreabilidade completa
- ğŸ”’ **Versionamento**: Time Travel habilitado para rollback
- ğŸ”’ **Data Lineage**: Rastreamento automÃ¡tico de transformaÃ§Ãµes

---

## ğŸ’° OtimizaÃ§Ã£o de Custos

> [!TIP]
> **EstratÃ©gias para Reduzir Custos Cloud**

### AWS Lambda
- âœ… **Memory optimization**: 512MB (ajustado para workload)
- âœ… **Reserved concurrency**: Evita custos inesperados
- âœ… **Timeout**: 60 segundos (just enough para download)
- ğŸ’¡ **Custo estimado**: < $1/mÃªs (execuÃ§Ã£o diÃ¡ria)

### Amazon S3
- âœ… **Lifecycle policies**: Mover dados antigos para S3 Glacier apÃ³s 90 dias
- âœ… **Intelligent-Tiering**: Para dados com padrÃµes de acesso variÃ¡veis
- âœ… **Particionamento eficiente**: Reduz custo de queries Athena/Databricks
- ğŸ’¡ **Custo estimado**: ~$1-3/mÃªs (para ~20GB/ano)

### Databricks
- âœ… **Cluster autoscaling**: Min 1 worker, Max 3 workers
- âœ… **Spot instances**: Economia de atÃ© 80% em workers
- âœ… **Auto-termination**: 15 minutos de inatividade
- âœ… **Job clusters**: Usar em produÃ§Ã£o (nÃ£o interactive)
- ğŸ’¡ **Custo estimado**: $50-100/mÃªs (depende de uso)

### RecomendaÃ§Ã£o para ProduÃ§Ã£o

> [!NOTE]
> **Infrastructure as Code**: Em produÃ§Ã£o, recomenda-se usar **AWS CloudFormation** ou **Terraform** para provisionamento automatizado. Este projeto foi configurado via console AWS para fins de laboratÃ³rio e prototipagem rÃ¡pida.

---

## ğŸ“Š MÃ©tricas de Performance

### Pipeline de IngestÃ£o (Lambda)
| MÃ©trica | Valor TÃ­pico |
|---------|--------------|
| **Execution Time** | ~15-25 segundos |
| **Memory Used** | ~300-400 MB |
| **Data Downloaded** | ~800 MB - 1.2 GB |
| **Upload to S3** | ~5-8 segundos |
| **Monthly Executions** | 30 (1x por dia) |

### Pipeline de Processamento (Databricks)
| Camada | Tempo MÃ©dio | Volume Processado |
|--------|-------------|-------------------|
| **Bronze** | ~2 min | 1.5M+ rows |
| **Silver** | ~3-5 min | 1.5M rows (apÃ³s limpeza) |
| **Gold** | ~1-2 min | ~1K aggregations |
| **Total Pipeline** | **~10 min** | **~1.5M rows** |

### OtimizaÃ§Ãµes Implementadas
- âœ… **Z-Ordering** por CNPJ_FUNDO (melhora queries em 40%)
- âœ… **Particionamento** por ano/mÃªs (reduz full scans)
- âœ… **Delta Lake OPTIMIZE** (compactaÃ§Ã£o de small files)
- âœ… **Broadcast joins** para tabelas pequenas

---

## ğŸ”§ Troubleshooting

### Problema: Lambda timeout ao baixar arquivos

**Sintoma**: FunÃ§Ã£o Lambda termina com timeout error  
**Causa**: Arquivo CVM muito grande ou conexÃ£o lenta  
**SoluÃ§Ã£o**:
```python
# Aumentar timeout da funÃ§Ã£o Lambda
# Em lambda configuration:
Timeout: 120 seconds (ao invÃ©s de 60)
```

### Problema: Erro de permissÃ£o S3

**Sintoma**: `AccessDenied` ao fazer upload no S3  
**Causa**: IAM Role da Lambda sem permissÃµes adequadas  
**SoluÃ§Ã£o**:
```json
{
  "Effect": "Allow",
  "Action": [
    "s3:PutObject",
    "s3:GetObject",
    "s3:ListBucket"
  ],
  "Resource": [
    "arn:aws:s3:::seu-bucket/*",
    "arn:aws:s3:::seu-bucket"
  ]
}
```

### Problema: Databricks nÃ£o consegue ler S3

**Sintoma**: `CredentialRetrievalException` no Databricks  
**Causa**: Credenciais AWS nÃ£o configuradas ou IAM Role ausente  
**SoluÃ§Ã£o (Recomendado - IAM Role)**:
```python
# Configure Instance Profile com IAM Role no cluster Databricks
# Databricks Admin Console > Cluster > Advanced Options > AWS IAM Role
spark.conf.set("fs.s3a.aws.credentials.provider", 
               "com.amazonaws.auth.InstanceProfileCredentialsProvider")
```

**SoluÃ§Ã£o Alternativa (Databricks Secrets)**:
```python
# Apenas se IAM Role nÃ£o for viÃ¡vel
spark.conf.set("fs.s3a.access.key", dbutils.secrets.get(scope="aws", key="access-key"))
spark.conf.set("fs.s3a.secret.key", dbutils.secrets.get(scope="aws", key="secret-key"))
```

### Problema: Schema evolution error na Silver

**Sintoma**: `AnalysisException: incompatible schema`  
**Causa**: MudanÃ§a de schema na fonte CVM  
**SoluÃ§Ã£o**:
```python
# Habilitar mergeSchema no read
df = spark.read.option("mergeSchema", "true").format("delta").load("path")
```

### Problema: Out of Memory no Databricks

**Sintoma**: `OutOfMemoryError` durante processamento  
**Causa**: Dataset grande + cluster small  
**SoluÃ§Ã£o**:
- Aumentar workers do cluster (2-3 workers)
- Usar `repartition()` para distribuir dados
- Processar em batches por mÃªs

---

## ğŸ”® PrÃ³ximos Passos

> [!NOTE]
> **EvoluÃ§Ã£o EstratÃ©gica do Projeto**

### IntegraÃ§Ã£o com CRM e AutomaÃ§Ã£o Comercial

**PrÃ³ximas implementaÃ§Ãµes planejadas:**

1. **ğŸ“§ Alertas AutomÃ¡ticos para Especialistas de Investimento**
   - Disparo de email quando cliente solicitar **portabilidade de saÃ­da**
   - NotificaÃ§Ã£o contÃ©m: dados do fundo, valor estimado, urgÃªncia
   - Permite aÃ§Ã£o rÃ¡pida de retenÃ§Ã£o

2. **ğŸ¯ IntegraÃ§Ã£o com CRM para CaptaÃ§Ã£o**
   - Usar informaÃ§Ãµes de clientes que solicitaram portabilidade **de entrada**
   - Acionar ferramentas de **blindagem de capital**
   - Criar tarefas automÃ¡ticas no CRM para equipe comercial

3. **ğŸ“Š Dashboards AnalÃ­ticos**
   - Power BI / Databricks SQL
   - VisualizaÃ§Ã£o de tendÃªncias de portabilidade
   - Indicadores de risco por fundo

4. **ğŸ”” OrquestraÃ§Ã£o Completa do Pipeline**
   - Apache Airflow ou Databricks Workflows
   - Monitoramento de falhas e alertas

5. **ğŸ“š CatÃ¡logo e GovernanÃ§a de Dados**
   - DocumentaÃ§Ã£o automÃ¡tica de schemas
   - Data lineage completo
   - PolÃ­ticas de acesso e privacidade

---

## ğŸŒŸ Diferenciais do Projeto

- âœ… **Problema real e regulatÃ³rio** (CVM 210)
- âœ… **SoluÃ§Ã£o criada mesmo com limitaÃ§Ãµes de acesso Ã  fonte**
- âœ… **Arquitetura moderna, escalÃ¡vel e alinhada ao mercado**
- âœ… **IntegraÃ§Ã£o entre AWS e Databricks**
- âœ… **Foco nÃ£o apenas tÃ©cnico, mas tambÃ©m estratÃ©gico e financeiro**
- âœ… **Projeto desenvolvido end-to-end, individualmente**
- âœ… **Alinhado Ã s competÃªncias esperadas de um Engenheiro de Dados SÃªnior**

---

## ğŸ‘¨â€ğŸ’» Sobre o Autor

Este projeto foi desenvolvido como demonstraÃ§Ã£o de competÃªncias tÃ©cnicas e estratÃ©gicas em **Engenharia de Dados**, cobrindo:

- ğŸ—ï¸ **Arquitetura de dados** (Medallion, Data Lake)
- â˜ï¸ **Cloud computing** (AWS)
- âš¡ **Processamento distribuÃ­do** (PySpark, Delta Lake)
- ğŸ“Š **GovernanÃ§a de dados**
- ğŸ’¼ **VisÃ£o de negÃ³cio** (insights acionÃ¡veis)

---

## ğŸ“„ LicenÃ§a

Este projeto Ã© de propriedade privada e foi desenvolvido para fins de demonstraÃ§Ã£o tÃ©cnica.

---

**Desenvolvido com â¤ï¸ e â˜• por um Engenheiro de Dados apaixonado por resolver problemas reais.**
