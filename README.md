# CVM 210 Data Pipeline

![Project Banner](assets/images/project_banner.png)

[![AWS](https://img.shields.io/badge/AWS-Lambda%20%7C%20S3-FF9900?style=for-the-badge&logo=amazon-aws&logoColor=white)](https://aws.amazon.com/)
[![Databricks](https://img.shields.io/badge/Databricks-Delta%20Lake-FF3621?style=for-the-badge&logo=databricks&logoColor=white)](https://databricks.com/)
[![Python](https://img.shields.io/badge/Python-3.9%2B-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://www.python.org/)
[![PySpark](https://img.shields.io/badge/PySpark-E25A1C?style=for-the-badge&logo=apache-spark&logoColor=white)](https://spark.apache.org/)
[![License](https://img.shields.io/badge/License-MIT-green?style=for-the-badge)](LICENSE)
[![Code Style](https://img.shields.io/badge/Code%20Style-Black-black?style=for-the-badge)](https://github.com/psf/black)

## ğŸ“‘ Ãndice

- [VisÃ£o Geral](#-visÃ£o-geral)
- [Objetivos do Projeto](#-objetivos-do-projeto)
- [Arquitetura da SoluÃ§Ã£o](#%EF%B8%8F-arquitetura-da-soluÃ§Ã£o)
- [Tecnologias Utilizadas](#-tecnologias-utilizadas)
- [Estrutura do Projeto](#-estrutura-do-projeto)
- [Como Executar](#-como-executar)
- [Exemplos de AnÃ¡lises](#-exemplos-de-anÃ¡lises)
- [GovernanÃ§a e Qualidade](#-regras-de-negÃ³cio--governanÃ§a)
- [OtimizaÃ§Ã£o de Custos](#-otimizaÃ§Ã£o-de-custos)
- [MÃ©tricas de Performance](#-mÃ©tricas-de-performance)
- [Troubleshooting](#-troubleshooting)
- [PrÃ³ximos Passos](#-prÃ³ximos-passos)
- [Diferenciais do Projeto](#-diferenciais-do-projeto)
- [Sobre o Autor](#-sobre-o-autor)
- [LicenÃ§a](#-licenÃ§a)

---

## ğŸ“Œ VisÃ£o Geral

Este projeto implementa uma **soluÃ§Ã£o completa de engenharia de dados** para capturar, processar e analisar informaÃ§Ãµes pÃºblicas da **CVM (ResoluÃ§Ã£o CVM 210)**, simulando um cenÃ¡rio real de mercado onde nÃ£o hÃ¡ API oficial disponÃ­vel para consumo direto dos dados regulatÃ³rios.

> [!IMPORTANT]
> **Contexto RegulatÃ³rio**: Embora a CVM disponibilize dados pÃºblicos em seu portal, **as informaÃ§Ãµes da ResoluÃ§Ã£o CVM 210 (portabilidade de investimentos) sÃ£o de acesso restrito** e nÃ£o estÃ£o disponÃ­veis publicamente. Este projeto demonstra a capacidade de construir uma arquitetura robusta para captura e anÃ¡lise desses dados em um ambiente corporativo real.

A proposta foi resolver um problema real de negÃ³cio, desde a **ingestÃ£o automatizada** atÃ© a **anÃ¡lise estratÃ©gica**, utilizando uma arquitetura moderna em cloud (**AWS + Databricks**) e boas prÃ¡ticas de Data Engineering em produÃ§Ã£o.

---

## ğŸ¯ Objetivos do Projeto

âœ… **Automatizar a captura diÃ¡ria** dos dados publicados pela CVM  
âœ… **Garantir persistÃªncia, histÃ³rico e rastreabilidade** das informaÃ§Ãµes  
âœ… **Transformar dados brutos em informaÃ§Ã£o analÃ­tica confiÃ¡vel**  
âœ… **Viabilizar anÃ¡lises de portabilidade e movimentaÃ§Ã£o de fundos**  
âœ… **Criar uma base escalÃ¡vel** para estratÃ©gias financeiras futuras

---

## ğŸ—ï¸ Arquitetura da SoluÃ§Ã£o

![Architecture Diagram](assets/images/architecture_diagram.png)

### Fluxo Completo de Dados

```mermaid
graph TB
    A[CVM Website<br/>inf_diario_fi_YYYYMM.zip] -->|HTTP Download| B[AWS Lambda<br/>Daily Trigger]
    B -->|Unzip + Upload| C[S3 Data Lake<br/>ano=YYYY/mes=MM/]
    C -->|PySpark Read| D{Databricks<br/>ELT Processing}
    D -->|Ingestion<br/>Schema on Read| E[(Bronze Layer<br/>Raw CSV Data)]
    E -->|Transformations<br/>Quality Checks| F[(Silver Layer<br/>Cleaned + Standardized)]
    F -->|Aggregations<br/>Business KPIs| G[(Gold Layer<br/>Analytics Ready)]
    G -->|SQL Queries| H[Portability Analysis<br/>Risk Detection]
    H -.->|Future: Alerts| I[CRM Integration<br/>Planned]
    I -.->|Future: Email| J[Investment Advisors<br/>Planned]
    
    style A fill:#e1f5ff,stroke:#0066cc,stroke-width:2px
    style B fill:#fff4e1,stroke:#ff9900,stroke-width:2px
    style C fill:#ffe1f5,stroke:#cc00cc,stroke-width:2px
    style D fill:#ffe1f5,stroke:#cc00cc,stroke-width:2px
    style E fill:#d4a574,stroke:#8b5a2b,stroke-width:2px
    style F fill:#c0c0c0,stroke:#666666,stroke-width:2px
    style G fill:#ffd700,stroke:#cc9900,stroke-width:2px
    style H fill:#c1f0c1,stroke:#00aa00,stroke-width:2px
    style I fill:#d4d4ff,stroke:#6666ff,stroke-width:2px,stroke-dasharray: 5 5
    style J fill:#f0e1ff,stroke:#9966cc,stroke-width:2px,stroke-dasharray: 5 5
```

> **Legenda**: Componentes com borda tracejada e setas pontilhadas representam funcionalidades planejadas (Future State).

### ğŸ”¹ IngestÃ£o de Dados

- **ExecuÃ§Ã£o diÃ¡ria via AWS Lambda**, alinhada ao horÃ¡rio de publicaÃ§Ã£o da CVM
- **ExtraÃ§Ã£o direta do site da CVM** (simulando ingestÃ£o via API de instituiÃ§Ãµes autorizadas)
- **GeraÃ§Ã£o de arquivos diÃ¡rios no Amazon S3** (Data Lake â€“ Raw Zone)
- **DescompactaÃ§Ã£o automÃ¡tica** de arquivos ZIP para CSV

### ğŸ”¹ Armazenamento

- **OrganizaÃ§Ã£o dos dados no S3** com separaÃ§Ã£o lÃ³gica por data (`ano=YYYY/mes=MM/`)
- **PersistÃªncia histÃ³rica** para auditoria e reprocessamento
- **Base preparada para schema evolution**

### ğŸ”„ Processamento & TransformaÃ§Ã£o (ELT)

**Arquitetura Medallion implementada no Databricks:**

![Medallion Flow](assets/images/medallion_flow.png)

#### **Bronze Layer** ğŸŸ¤
- Dados brutos ingeridos diretamente do S3
- Schema on read
- Sem transformaÃ§Ãµes
- HistÃ³rico completo preservado

#### **Silver Layer** âšª
- **Limpeza e padronizaÃ§Ã£o** de dados
- **Tratamento de colunas** (compatibilidade CVM 175 vs formato antigo)
- **ValidaÃ§Ãµes de Data Quality** (valores vÃ¡lidos, campos obrigatÃ³rios)
- **DeduplicaÃ§Ã£o** via merge (garantia de nÃ£o duplicidade)
- **Schema evolution** automÃ¡tico
- **Z-Order optimization** por CNPJ_FUNDO

#### **Gold Layer** ğŸŸ¡
- **AgregaÃ§Ãµes por fundo e perÃ­odo**
- **KPIs de negÃ³cio calculados:**
  - Total de captaÃ§Ãµes
  - Total de resgates
  - **Fluxo lÃ­quido** (captaÃ§Ã£o - resgate)
  - PatrimÃ´nio mÃ©dio
  - VariaÃ§Ã£o de cota
  - **Indicadores de portabilidade**
- **Dados prontos para BI e anÃ¡lises**

### ğŸ”§ Tecnologias Utilizadas

| Camada | Tecnologia | PropÃ³sito |
|--------|-----------|-----------|
| **IngestÃ£o** | AWS Lambda + Python | AutomaÃ§Ã£o de captura diÃ¡ria |
| **Storage** | Amazon S3 | Data Lake (Raw Zone) |
| **Processing** | Databricks + PySpark | Processamento distribuÃ­do |
| **Data Format** | Delta Lake | Versionamento, ACID, Time Travel |
| **Orchestration** | AWS EventBridge (trigger diÃ¡rio) | Agendamento |
| **Governance** | Unity Catalog | CatÃ¡logo de metadados |

---

## ğŸ“‚ Estrutura do Projeto

```
eng-dados-project/
â”‚
â”œâ”€â”€ assets/
â”‚   â””â”€â”€ images/                   # Imagens do projeto (banner, diagramas)
â”‚       â”œâ”€â”€ project_banner.png
â”‚       â”œâ”€â”€ architecture_diagram.png
â”‚       â””â”€â”€ medallion_flow.png
â”‚
â”œâ”€â”€ notebooks/                     # Jupyter Notebooks
â”‚   â”œâ”€â”€ pipeline_principal.ipynb  # Pipeline completo (Bronze â†’ Silver â†’ Gold)
â”‚   â”œâ”€â”€ data_processing.ipynb     # Processamento e transformaÃ§Ãµes
â”‚   â””â”€â”€ analytics.ipynb           # AnÃ¡lises de portabilidade e insights
â”‚
â”œâ”€â”€ src/                          # CÃ³digo Python reutilizÃ¡vel
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ utils.py                  # FunÃ§Ãµes utilitÃ¡rias (formataÃ§Ã£o, validaÃ§Ã£o)
â”‚   â””â”€â”€ s3_helper.py              # Helper para operaÃ§Ãµes S3
â”‚
â”œâ”€â”€ docs/                         # DocumentaÃ§Ã£o tÃ©cnica
â”‚   â”œâ”€â”€ lambda_ingestion.md      # Detalhes da ingestÃ£o Lambda
â”‚   â”œâ”€â”€ data_pipeline.md         # Detalhes do pipeline ELT
â”‚   â””â”€â”€ analytics_guide.md       # Guia de anÃ¡lises disponÃ­veis
â”‚
â”œâ”€â”€ lambda_function.py            # FunÃ§Ã£o Lambda para ingestÃ£o diÃ¡ria
â”‚
â”œâ”€â”€ .gitignore                    # Arquivos ignorados pelo Git
â”œâ”€â”€ .env.example                  # Template de variÃ¡veis de ambiente
â”œâ”€â”€ requirements.txt              # DependÃªncias Python
â”œâ”€â”€ LICENSE                       # LicenÃ§a MIT
â””â”€â”€ README.md                     # Este arquivo
```

---

## ğŸš€ Como Executar

### 1ï¸âƒ£ ConfiguraÃ§Ã£o da IngestÃ£o (AWS Lambda)

```bash
# Deploy da funÃ§Ã£o Lambda
aws lambda create-function \
  --function-name cvm210-daily-ingestion \
  --runtime python3.9 \
  --role <IAM_ROLE_ARN> \
  --handler lambda_function.lambda_handler \
  --zip-file fileb://lambda_function.zip

# Configurar trigger diÃ¡rio (EventBridge)
aws events put-rule \
  --name cvm210-daily-trigger \
  --schedule-expression "cron(0 20 * * ? *)"
```

### 2ï¸âƒ£ Processamento no Databricks

1. **Configurar credenciais AWS** no Databricks (via Secrets ou IAM Role)
2. **Executar notebook `notebooks/pipeline_principal.ipynb`** para criar as camadas Bronze/Silver/Gold
3. **Executar notebook `notebooks/analytics.ipynb`** para anÃ¡lises de portabilidade

### 3ï¸âƒ£ Consultar Dados AnalÃ­ticos

```sql
-- Exemplo: Fundos com maior saÃ­da de capital (portabilidade)
SELECT 
  CNPJ_FUNDO,
  total_captacao,
  total_resgate,
  fluxo_liquido,
  patrimonio_medio
FROM cvm_p210.gold_cvm210_analytics
WHERE fluxo_liquido < 0
ORDER BY fluxo_liquido ASC
LIMIT 10;
```

---

## ğŸ“Š Exemplos de AnÃ¡lises

### AnÃ¡lise de Portabilidade

O projeto gera **insights acionÃ¡veis** para o time de negÃ³cios:

- **Fundos em risco** (com fluxo lÃ­quido negativo)
- **Volume total movimentado** por tipo de portabilidade
- **TendÃªncias de captaÃ§Ã£o vs resgate**

**Exemplo de output:**

| CNPJ_FUNDO | Total CaptaÃ§Ã£o | Total Resgate | Fluxo LÃ­quido | PatrimÃ´nio MÃ©dio |
|------------|----------------|---------------|---------------|------------------|
| 12.345.678 | R$ 1.2M | R$ 2.5M | **-R$ 1.3M** | R$ 450M |
| 23.456.789 | R$ 800K | R$ 1.1M | **-R$ 300K** | R$ 120M |

---

## ğŸ§  Regras de NegÃ³cio & GovernanÃ§a

### Data Quality
- âœ… ValidaÃ§Ã£o de **campos obrigatÃ³rios** (CNPJ_FUNDO, DT_COMPTC)
- âœ… Filtro de **patrimÃ´nio lÃ­quido > 0**
- âœ… **DeduplicaÃ§Ã£o automÃ¡tica** via merge

### Metadados e Rastreabilidade
- âœ… Timestamp de processamento em cada camada
- âœ… Particionamento por **ano/mÃªs**
- âœ… Versionamento via **Delta Lake**

### SeguranÃ§a & GovernanÃ§a
- ğŸ”’ **Credenciais AWS**: IAM Roles via Instance Profile (production-ready)
- ğŸ”’ **Armazenamento**: Unity Catalog para controle de acesso
- ğŸ”’ **Auditoria**: Delta Lake transaction log para rastreabilidade completa
- ğŸ”’ **Versionamento**: Time Travel habilitado para rollback
- ğŸ”’ **Data Lineage**: Rastreamento automÃ¡tico de transformaÃ§Ãµes

---

## ğŸ’° OtimizaÃ§Ã£o de Custos (Resumo Executivo)

ğŸ’° **Estimated annual savings**: For 10,000 queries/year, savings of **~$137** (from $172 to $35).

Este ganho foi obtido principalmente atravÃ©s de:
- **Uso de formatos colunares (Parquet)**: ReduÃ§Ã£o drÃ¡stica na quantidade de dados lidos.
- **CompressÃ£o Snappy**: EquilÃ­brio perfeito entre taxa de compressÃ£o e velocidade de leitura.
- **Arquitetura Medallion**: ReduÃ§Ã£o de scans desnecessÃ¡rios ao consultar camadas refinadas.
- **Processamento Serverless / Sob Demanda**: Uso eficiente de AWS Lambda e clusters Databricks com auto-termination.

---

## ğŸ“Š MÃ©tricas de Performance

### Pipeline de IngestÃ£o (Lambda)
| MÃ©trica | Valor TÃ­pico |
|---------|--------------|
| **Execution Time** | ~15-25 segundos |
| **Memory Used** | ~300-400 MB |
| **Data Downloaded** | ~800 MB - 1.2 GB |
| **Upload to S3** | ~5-8 segundos |

### Pipeline de Processamento (Databricks)
| Camada | Tempo MÃ©dio | Volume Processado |
|--------|-------------|-------------------|
| **Bronze** | ~2 min | 1.5M+ rows |
| **Silver** | ~3-5 min | 1.5M rows (apÃ³s limpeza) |
| **Gold** | ~1-2 min | ~1K aggregations |
| **Total Pipeline** | **~10 min** | **~1.5M rows** |

---

## ğŸ” Key Generated Insights

O pipeline demonstra como a arquitetura suporta anÃ¡lises complexas com baixo custo e alta performance, habilitando decisÃµes estratÃ©gicas:

âœ… **Portability Trends**: DetecÃ§Ã£o de fundos com tendÃªncia elevada de saÃ­da de capital para retenÃ§Ã£o preventiva.
âœ… **Concentration Risk**: IdentificaÃ§Ã£o de ativos com alta concentraÃ§Ã£o em poucos investidores, mitigando riscos sistÃªmicos.
âœ… **Market Resilience**: AnÃ¡lise de como as cotas e o patrimÃ´nio reagiram a eventos de mercado especÃ­ficos.
âœ… **Regulatory Compliance**: Garantia de integridade e consistÃªncia dos dados conforme a ResoluÃ§Ã£o CVM 210.
âœ… **Economic Impact**: CorrelaÃ§Ã£o entre movimentaÃ§Ãµes de mercado e variaÃ§Ãµes no fluxo lÃ­quido de fundos especÃ­ficos.

---

## ğŸ§  Technical Decisions

### Why Medallion Architecture?
| Layer | Purpose | Benefit |
|-------|---------|---------|
| **Bronze** | Immutable raw data | Audit and reprocessing |
| **Silver** | Clean and structured data | Quality and consistency |
| **Gold** | Business metrics | Query performance |

### Why Databricks instead of EMR or Glue?
- âœ… **Managed Service**: Facilidade de colaboraÃ§Ã£o e menor overhead operacional.
- âœ… **Delta Lake Features**: ACID transactions, Time Travel e Schema Evolution nativos.
- âœ… **Cost-effective**: Auto-termination e Spot Instances reduzem custos significativamente.
- âœ… **Performance**: Photon engine otimiza o processamento distribuÃ­do.

### Why Parquet + Snappy?
- **Parquet**: Formato colunar (selective scan) que lÃª apenas as colunas necessÃ¡rias para a query.
- **Snappy**: Velocidade de descompressÃ£o 2-3x mais rÃ¡pida que GZIP, ideal para processamento em tempo real.
- **Cost-benefit**: Ocupa ~60% do tamanho de um CSV GZIP, mas permite queries muito mais rÃ¡pidas e baratas.

---

## ğŸ‘¨â€ğŸ’» Autor

**Gabriel Henrique - Data Engineer**
ğŸ“ Data Engineering Post-Graduate Student | **FIAP**
ğŸ’¼ Specialized in Modern Data Architectures on AWS (S3, Lambda, Databricks, PySpark)
ğŸš€ Experience with ELT pipelines, Medallion Architecture, and performance optimization

ï¿½ *Open to opportunities in Data Engineering, Analytics Engineering and Cloud Data Platforms.*

---

## ğŸ“ LicenÃ§a

Este projeto Ã© parte de um trabalho acadÃªmico (**Tech Challenge - FIAP Post-Graduate Program**).  
Dados pÃºblicos fornecidos pela **CVM (ComissÃ£o de Valores MobiliÃ¡rios)**.

---

## ğŸ™ Agradecimentos

- **CVM** por disponibilizar microdados financeiros pÃºblicos.
- **FIAP** pelo ambiente de aprendizado focado em desafios prÃ¡ticos do mercado.
- **AWS** pela documentaÃ§Ã£o completa e ferramentas poderosas de Engenharia de Dados.

â­ *If this project was useful, consider giving it a star on the repository!*

**Developed with â¤ï¸ using AWS Lambda, S3, Databricks and PySpark.**
> [!NOTE]
> Este projeto foi desenvolvido em ambiente de laboratÃ³rio AWS Academy para fins educacionais. 
> Os recursos demonstrados foram provisionados temporariamente e posteriormente removidos.

**Desenvolvido com â¤ï¸ e â˜• por um Engenheiro de Dados apaixonado por resolver problemas reais.**
